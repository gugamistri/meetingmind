# Cross-Platform Testing Framework for Story 1.3

## Automated Testing Infrastructure

### GitHub Actions Matrix Strategy

```yaml
# .github/workflows/transcription-testing.yml
name: Transcription Pipeline Cross-Platform Testing

on:
  push:
    paths: 
      - 'src-tauri/src/transcription/**'
      - 'src/components/transcription/**'
  pull_request:
    paths:
      - 'src-tauri/src/transcription/**'

jobs:
  cross-platform-tests:
    strategy:
      matrix:
        os: [ubuntu-latest, windows-latest, macos-latest, macos-13] # Intel and Apple Silicon
        hardware-profile: [minimum, recommended]
        include:
          - os: ubuntu-latest
            target: x86_64-unknown-linux-gnu
          - os: windows-latest  
            target: x86_64-pc-windows-msvc
          - os: macos-latest
            target: aarch64-apple-darwin
          - os: macos-13
            target: x86_64-apple-darwin
            
    runs-on: ${{ matrix.os }}
    steps:
      - name: Simulate hardware constraints
        if: matrix.hardware-profile == 'minimum'
        run: |
          # Limit memory and CPU for minimum spec testing
          echo "Testing under minimum hardware constraints"
```

### Test Data Management

#### Audio Sample Repository Structure
```
tests/fixtures/audio/
├── clear-speech/
│   ├── english-male-30s.wav
│   ├── english-female-30s.wav  
│   ├── portuguese-male-30s.wav
│   └── portuguese-female-30s.wav
├── background-noise/
│   ├── office-environment-60s.wav
│   ├── cafe-background-30s.wav
│   └── traffic-noise-30s.wav
├── challenging/
│   ├── multiple-speakers-overlap.wav
│   ├── low-quality-phone.wav
│   └── whispered-speech.wav
└── performance/
    ├── stress-test-5min.wav
    ├── stress-test-30min.wav
    └── concurrent-test-samples/
```

#### Ground Truth Transcriptions
```
tests/fixtures/transcriptions/
├── clear-speech/
│   ├── english-male-30s.txt
│   ├── english-female-30s.txt
│   └── accuracy-benchmarks.json
└── performance/
    └── latency-benchmarks.json
```

### Performance Testing Framework

#### Rust Backend Performance Tests
```rust
// src-tauri/src/transcription/benches/latency_benchmarks.rs
use criterion::{criterion_group, criterion_main, Criterion};
use transcription::WhisperService;

fn benchmark_model_loading(c: &mut Criterion) {
    c.bench_function("whisper_tiny_model_loading", |b| {
        b.iter(|| {
            let service = WhisperService::new("tiny").unwrap();
            service.load_model()
        })
    });
}

fn benchmark_inference_latency(c: &mut Criterion) {
    let service = WhisperService::new("tiny").unwrap();
    let audio_sample = load_test_audio("clear-speech/english-male-30s.wav");
    
    c.bench_function("30s_audio_inference", |b| {
        b.iter(|| {
            service.transcribe(audio_sample.clone())
        })
    });
}

criterion_group!(benches, benchmark_model_loading, benchmark_inference_latency);
criterion_main!(benches);
```

#### Memory Profiling Tests
```rust
// tests/integration/memory_profiling.rs
#[tokio::test]
async fn test_memory_usage_during_extended_session() {
    let initial_memory = get_memory_usage();
    let service = TranscriptionService::new().await?;
    
    // Process 60 minutes of audio in 30-second chunks
    for chunk in audio_chunks_60min() {
        service.process_chunk(chunk).await?;
        
        let current_memory = get_memory_usage();
        assert!(current_memory - initial_memory < 512_000_000); // <512MB growth
    }
    
    // Verify memory is released after processing
    drop(service);
    tokio::time::sleep(Duration::from_secs(5)).await;
    
    let final_memory = get_memory_usage();
    assert!(final_memory <= initial_memory + 50_000_000); // Allow 50MB baseline
}
```

### Cross-Platform Compatibility Validation

#### Platform Detection Testing
```rust
// tests/integration/platform_compatibility.rs
#[tokio::test]
async fn test_cross_platform_model_loading() {
    let platforms = [
        Platform::Windows,
        Platform::MacOSIntel, 
        Platform::MacOSAppleSilicon,
        Platform::Linux
    ];
    
    for platform in platforms {
        let config = PlatformConfig::for_platform(platform);
        let service = WhisperService::with_config(config).await;
        
        assert!(service.is_ok(), "Model loading failed on {:?}", platform);
        
        let transcription = service?
            .transcribe(load_test_audio("clear-speech/english-male-30s.wav"))
            .await?;
            
        assert!(transcription.confidence > 0.8);
        assert!(transcription.latency_ms < 3000);
    }
}
```

#### Audio Format Compatibility
```rust
#[tokio::test] 
async fn test_audio_format_preprocessing_consistency() {
    let input_formats = ["wav", "mp3", "m4a", "flac"];
    let reference_transcription = load_reference("english-male-30s.txt");
    
    for format in input_formats {
        let audio_file = format!("clear-speech/english-male-30s.{}", format);
        let result = transcription_service
            .transcribe(load_test_audio(&audio_file))
            .await?;
            
        let similarity = calculate_text_similarity(&result.text, &reference_transcription);
        assert!(similarity > 0.95, "Format {} produced inconsistent results", format);
    }
}
```

## Quality Metrics and Monitoring

### Accuracy Measurement Framework
```rust
// src-tauri/src/transcription/accuracy.rs
pub struct AccuracyMetrics {
    pub word_error_rate: f64,
    pub character_error_rate: f64, 
    pub confidence_correlation: f64,
    pub language_detection_accuracy: f64,
}

impl AccuracyMetrics {
    pub fn calculate(predicted: &str, ground_truth: &str) -> Self {
        // Implement WER, CER, and other accuracy metrics
    }
}

#[tokio::test]
async fn validate_transcription_accuracy_requirements() {
    let test_cases = load_accuracy_test_cases();
    let mut total_wer = 0.0;
    
    for case in test_cases {
        let result = transcription_service.transcribe(case.audio).await?;
        let metrics = AccuracyMetrics::calculate(&result.text, &case.ground_truth);
        
        assert!(metrics.word_error_rate < 0.2, "WER too high: {}", metrics.word_error_rate);
        total_wer += metrics.word_error_rate;
    }
    
    let average_wer = total_wer / test_cases.len() as f64;
    assert!(average_wer < 0.15, "Average WER requirement not met: {}", average_wer);
}
```

### Performance Monitoring Integration
```rust
// src-tauri/src/transcription/monitoring.rs
pub struct PerformanceMonitor {
    latency_tracker: LatencyTracker,
    memory_tracker: MemoryTracker,
    accuracy_tracker: AccuracyTracker,
}

impl PerformanceMonitor {
    pub fn track_transcription_event(&mut self, event: TranscriptionEvent) {
        match event {
            TranscriptionEvent::Started { timestamp, audio_duration } => {
                self.latency_tracker.start_timing(timestamp, audio_duration);
            }
            TranscriptionEvent::Completed { result, timestamp } => {
                self.latency_tracker.end_timing(timestamp);
                self.accuracy_tracker.record_result(result);
                
                // Assert performance requirements in tests
                #[cfg(test)]
                {
                    let latency = self.latency_tracker.latest_latency();
                    assert!(latency < Duration::from_secs(3));
                }
            }
        }
    }
}
```

## Automated Test Execution Strategy

### Test Categories and Execution Frequency

#### Continuous Integration (Every Commit)
- Unit tests for transcription logic
- Fast integration tests with mocked ONNX
- Code coverage validation
- Static analysis (Clippy, ESLint)

#### Nightly Builds (Full Platform Matrix)
- Cross-platform integration tests
- Performance benchmarking
- Memory profiling validation
- Accuracy regression testing

#### Release Candidates (Comprehensive Validation)
- Full E2E testing with real models
- Extended stress testing
- Security and privacy validation
- Documentation and API contract testing

### Test Environment Management

#### Docker-based Testing
```dockerfile
# tests/docker/ubuntu-test-env/Dockerfile
FROM ubuntu:22.04

RUN apt-get update && apt-get install -y \
    build-essential \
    pkg-config \
    libasound2-dev \
    libssl-dev \
    && rm -rf /var/lib/apt/lists/*

# Install specific ONNX Runtime version for consistency
RUN wget https://github.com/microsoft/onnxruntime/releases/download/v1.16.0/onnxruntime-linux-x64-1.16.0.tgz
RUN tar -xzf onnxruntime-linux-x64-1.16.0.tgz

COPY ./test-data /test-data
COPY ./whisper-models /whisper-models

CMD ["cargo", "test", "--features", "integration-tests"]
```

### Continuous Monitoring and Alerting

#### Performance Regression Detection
```rust
// tests/performance/regression_detection.rs
#[tokio::test]
async fn detect_performance_regression() {
    let baseline_metrics = load_baseline_performance_metrics();
    let current_metrics = run_performance_benchmark().await?;
    
    // Allow 5% degradation tolerance
    assert!(
        current_metrics.avg_latency <= baseline_metrics.avg_latency * 1.05,
        "Performance regression detected: {} ms vs {} ms baseline",
        current_metrics.avg_latency,
        baseline_metrics.avg_latency
    );
    
    assert!(
        current_metrics.memory_usage <= baseline_metrics.memory_usage * 1.05,
        "Memory usage regression detected"
    );
}
```

This framework provides comprehensive automated testing infrastructure that ensures the critical risks (TECH-001 and PERF-001) are continuously validated throughout development and deployment.